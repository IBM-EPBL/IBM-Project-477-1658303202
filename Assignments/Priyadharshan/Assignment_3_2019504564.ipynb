{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c7747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "172e2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e68152d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938b4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43543137",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, vertical_flip=False, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea28c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3457 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train = datagen.flow_from_directory(r'C:\\Users\\spdpr\\Downloads\\flowers', target_size=(64,64),batch_size=32, class_mode='categorical', subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88568510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e96774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = x_train.class_indices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768e92c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 860 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "x_val = datagen.flow_from_directory(r'C:\\Users\\spdpr\\Downloads\\flowers', target_size=(64,64),batch_size=32, class_mode='categorical', subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4975ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b36715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPooling2D,Dense,Flatten,Convolution2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3d04303",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cb1f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c9a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87daab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50cf16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(128,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ad5a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "218a471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=300, kernel_initializer='random_uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e972fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=200, kernel_initializer='random_uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f8b3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=5,kernel_initializer='random_uniform',activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c255c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "710399f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               1382700   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 1005      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,537,153\n",
      "Trainable params: 1,537,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "beb3e7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\spdpr\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "40/40 [==============================] - 17s 380ms/step - loss: 1.5273 - accuracy: 0.2778 - val_loss: 1.3131 - val_accuracy: 0.3938\n",
      "Epoch 2/25\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 1.2867 - accuracy: 0.4019 - val_loss: 1.2508 - val_accuracy: 0.3938\n",
      "Epoch 3/25\n",
      "40/40 [==============================] - 14s 349ms/step - loss: 1.2319 - accuracy: 0.4516 - val_loss: 1.1553 - val_accuracy: 0.4969\n",
      "Epoch 4/25\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.1691 - accuracy: 0.5004 - val_loss: 1.1993 - val_accuracy: 0.4688\n",
      "Epoch 5/25\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.1010 - accuracy: 0.5547 - val_loss: 1.0948 - val_accuracy: 0.5312\n",
      "Epoch 6/25\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.0307 - accuracy: 0.5867 - val_loss: 1.0939 - val_accuracy: 0.5813\n",
      "Epoch 7/25\n",
      "40/40 [==============================] - 12s 308ms/step - loss: 0.9989 - accuracy: 0.6031 - val_loss: 1.0756 - val_accuracy: 0.5188\n",
      "Epoch 8/25\n",
      "40/40 [==============================] - 9s 220ms/step - loss: 0.9527 - accuracy: 0.6297 - val_loss: 1.0640 - val_accuracy: 0.5719\n",
      "Epoch 9/25\n",
      "40/40 [==============================] - 9s 218ms/step - loss: 0.9599 - accuracy: 0.6273 - val_loss: 0.9842 - val_accuracy: 0.6094\n",
      "Epoch 10/25\n",
      "40/40 [==============================] - 9s 218ms/step - loss: 0.9703 - accuracy: 0.6133 - val_loss: 1.0102 - val_accuracy: 0.6094\n",
      "Epoch 11/25\n",
      "40/40 [==============================] - 9s 222ms/step - loss: 0.8912 - accuracy: 0.6703 - val_loss: 0.8741 - val_accuracy: 0.6719\n",
      "Epoch 12/25\n",
      "40/40 [==============================] - 9s 221ms/step - loss: 0.8270 - accuracy: 0.6859 - val_loss: 0.9529 - val_accuracy: 0.6125\n",
      "Epoch 13/25\n",
      "40/40 [==============================] - 9s 216ms/step - loss: 0.8351 - accuracy: 0.6717 - val_loss: 1.0848 - val_accuracy: 0.5531\n",
      "Epoch 14/25\n",
      "40/40 [==============================] - 9s 217ms/step - loss: 0.9744 - accuracy: 0.6125 - val_loss: 0.9612 - val_accuracy: 0.5875\n",
      "Epoch 15/25\n",
      "40/40 [==============================] - 10s 246ms/step - loss: 0.8755 - accuracy: 0.6677 - val_loss: 0.9291 - val_accuracy: 0.6062\n",
      "Epoch 16/25\n",
      "40/40 [==============================] - 9s 229ms/step - loss: 0.8193 - accuracy: 0.6945 - val_loss: 0.9217 - val_accuracy: 0.6375\n",
      "Epoch 17/25\n",
      "40/40 [==============================] - 10s 263ms/step - loss: 0.8017 - accuracy: 0.6984 - val_loss: 0.8865 - val_accuracy: 0.6594\n",
      "Epoch 18/25\n",
      "40/40 [==============================] - 10s 249ms/step - loss: 0.8055 - accuracy: 0.6992 - val_loss: 0.8866 - val_accuracy: 0.6687\n",
      "Epoch 19/25\n",
      "40/40 [==============================] - 10s 243ms/step - loss: 0.7708 - accuracy: 0.7094 - val_loss: 0.8153 - val_accuracy: 0.6719\n",
      "Epoch 20/25\n",
      "40/40 [==============================] - 11s 281ms/step - loss: 0.7270 - accuracy: 0.7234 - val_loss: 0.9195 - val_accuracy: 0.6219\n",
      "Epoch 21/25\n",
      "40/40 [==============================] - 11s 265ms/step - loss: 0.7572 - accuracy: 0.7156 - val_loss: 0.9728 - val_accuracy: 0.6375\n",
      "Epoch 22/25\n",
      "40/40 [==============================] - 12s 290ms/step - loss: 0.7068 - accuracy: 0.7430 - val_loss: 0.9038 - val_accuracy: 0.6375\n",
      "Epoch 23/25\n",
      "40/40 [==============================] - 11s 278ms/step - loss: 0.8054 - accuracy: 0.6982 - val_loss: 0.9059 - val_accuracy: 0.6719\n",
      "Epoch 24/25\n",
      "40/40 [==============================] - 11s 268ms/step - loss: 0.7849 - accuracy: 0.6992 - val_loss: 0.8948 - val_accuracy: 0.6750\n",
      "Epoch 25/25\n",
      "40/40 [==============================] - 10s 256ms/step - loss: 0.7892 - accuracy: 0.6950 - val_loss: 0.8992 - val_accuracy: 0.6625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f6cd823848>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(x_train,steps_per_epoch=40,epochs=25,validation_data=x_val,validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5db2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = image.load_img(r\"C:\\Users\\spdpr\\Downloads\\red-rose-with-green-leaf_43623-944.jpg\", target_size=(64,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49833373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a420abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = image.img_to_array(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47589725",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = np.expand_dims(test_img,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "caa7b9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cef92c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_img)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ff726da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of flower is:  rose\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred)):\n",
    "    if pred[i]==1:\n",
    "        print('The type of flower is: ',classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97ca220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c82b6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"flower_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96cb308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5378160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"flower_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a7201216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d789139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
